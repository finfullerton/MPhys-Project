{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "limited interfernece: 10000 unlimited interference: 32000\n",
      "limited SM: 6000 unlimited SM: 10000\n",
      "Plus 1: 42000\n",
      "Event yield is: 0.085141905 .\n",
      "Event yield is: 0.00022501772 .\n",
      "Event yield is: 0.050693575 .\n",
      "Number of events: 4930\n",
      "[[-0.53947717  0.09341243  0.         ...  0.53818232 -1.5508709\n",
      "  -0.1619757 ]\n",
      " [ 0.37019989  2.59569883  2.77915931 ... -2.83929348  0.91344833\n",
      "  -0.64193505]\n",
      " [-1.56563044  3.99875355 -1.3536042  ... -1.16471696 -0.67853159\n",
      "   2.21454167]\n",
      " ...\n",
      " [ 1.82647002  1.00353456 -0.02592918 ...  0.79203051  2.02913833\n",
      "  -0.88427973]\n",
      " [-1.27396798 -1.48632562  0.         ... -1.84640718  0.0183014\n",
      "   2.36127353]\n",
      " [-0.52108467 -1.32072711  0.         ... -0.4316304   0.05273717\n",
      "  -0.56548917]]\n",
      "Reloading Tuner from keras_tuner_2/keras_training/tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/finfullerton/mambaforge/envs/mphys/lib/python3.11/site-packages/keras/src/saving/saving_lib.py:713: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "A total of 1 objects could not be loaded. Example error message for object <Dense name=dense_1, built=True>:\n\nThe shape of the target variable and the shape of the target value in `variable.assign(value)` must match. variable.shape=(385, 2), Received: value.shape=(385, 1). Target variable: <KerasVariable shape=(385, 2), dtype=float32, path=sequential/dense_1/kernel>\n\nList of objects that could not be loaded:\n[<Dense name=dense_1, built=True>]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 470\u001b[0m\n\u001b[1;32m    467\u001b[0m     plot_ML_observable(ML_results_1, weight_data_1)\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 470\u001b[0m     \u001b[43m__main__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[41], line 466\u001b[0m, in \u001b[0;36m__main__\u001b[0;34m()\u001b[0m\n\u001b[1;32m    463\u001b[0m plus_final_filtered_events_2, plus_final_filtered_weights_2 \u001b[38;5;241m=\u001b[39m jet_cuts(plus_filtered_events_2, plus_events_2, filtered_weights_2, plus_k_factor, plus_cut_flow_counts_2)\n\u001b[1;32m    465\u001b[0m x_data_1, y_data_1, weight_data_1 \u001b[38;5;241m=\u001b[39m root_variable_selector(plus_final_filtered_events_1, plus_final_filtered_weights_1)\n\u001b[0;32m--> 466\u001b[0m ML_results_1 \u001b[38;5;241m=\u001b[39m \u001b[43mML_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_data_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_data_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_data_1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m plot_ML_observable(ML_results_1, weight_data_1)\n",
      "Cell \u001b[0;32mIn[41], line 318\u001b[0m, in \u001b[0;36mML_training\u001b[0;34m(x_data, y_data, weight_data)\u001b[0m\n\u001b[1;32m    309\u001b[0m model \u001b[38;5;241m=\u001b[39m keras_tuner\u001b[38;5;241m.\u001b[39mRandomSearch(\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m hp: build_ML_model(hp, x_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:]),\n\u001b[1;32m    311\u001b[0m     objective\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    314\u001b[0m     project_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeras_training\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    315\u001b[0m )\n\u001b[1;32m    317\u001b[0m model\u001b[38;5;241m.\u001b[39msearch(x_train, y_train, sample_weight\u001b[38;5;241m=\u001b[39mtrain_weights, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39m(x_test, y_test, test_weights))\n\u001b[0;32m--> 318\u001b[0m best_model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_best_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_models\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    319\u001b[0m best_model\u001b[38;5;241m.\u001b[39mevaluate(x_test, y_test, sample_weight\u001b[38;5;241m=\u001b[39mtest_weights, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    321\u001b[0m best_hp \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_best_hyperparameters(num_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/mambaforge/envs/mphys/lib/python3.11/site-packages/keras_tuner/src/engine/tuner.py:400\u001b[0m, in \u001b[0;36mTuner.get_best_models\u001b[0;34m(self, num_models)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the best model(s), as determined by the tuner's objective.\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \n\u001b[1;32m    384\u001b[0m \u001b[38;5;124;03mThe models are loaded with the weights corresponding to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;124;03m    List of trained model instances sorted from the best to the worst.\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;66;03m# Method only exists in this class for the docstring override.\u001b[39;00m\n\u001b[0;32m--> 400\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_best_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_models\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/mphys/lib/python3.11/site-packages/keras_tuner/src/engine/base_tuner.py:366\u001b[0m, in \u001b[0;36mBaseTuner.get_best_models\u001b[0;34m(self, num_models)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the best model(s), as determined by the objective.\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \n\u001b[1;32m    353\u001b[0m \u001b[38;5;124;03mThis method is for querying the models trained during the search.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;124;03m    List of trained models sorted from the best to the worst.\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    365\u001b[0m best_trials \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mget_best_trials(num_models)\n\u001b[0;32m--> 366\u001b[0m models \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbest_trials\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m models\n",
      "File \u001b[0;32m~/mambaforge/envs/mphys/lib/python3.11/site-packages/keras_tuner/src/engine/base_tuner.py:366\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the best model(s), as determined by the objective.\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \n\u001b[1;32m    353\u001b[0m \u001b[38;5;124;03mThis method is for querying the models trained during the search.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;124;03m    List of trained models sorted from the best to the worst.\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    365\u001b[0m best_trials \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mget_best_trials(num_models)\n\u001b[0;32m--> 366\u001b[0m models \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m trial \u001b[38;5;129;01min\u001b[39;00m best_trials]\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m models\n",
      "File \u001b[0;32m~/mambaforge/envs/mphys/lib/python3.11/site-packages/keras_tuner/src/engine/tuner.py:331\u001b[0m, in \u001b[0;36mTuner.load_model\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;66;03m# Reload best checkpoint.\u001b[39;00m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;66;03m# Only load weights to avoid loading `custom_objects`.\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m maybe_distribute(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribution_strategy):\n\u001b[0;32m--> 331\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_checkpoint_fname\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrial_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/mambaforge/envs/mphys/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/mambaforge/envs/mphys/lib/python3.11/site-packages/keras/src/saving/saving_lib.py:593\u001b[0m, in \u001b[0;36m_raise_loading_failure\u001b[0;34m(error_msgs, warn_only)\u001b[0m\n\u001b[1;32m    591\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(msg)\n\u001b[1;32m    592\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 593\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[0;31mValueError\u001b[0m: A total of 1 objects could not be loaded. Example error message for object <Dense name=dense_1, built=True>:\n\nThe shape of the target variable and the shape of the target value in `variable.assign(value)` must match. variable.shape=(385, 2), Received: value.shape=(385, 1). Target variable: <KerasVariable shape=(385, 2), dtype=float32, path=sequential/dense_1/kernel>\n\nList of objects that could not be loaded:\n[<Dense name=dense_1, built=True>]"
     ]
    }
   ],
   "source": [
    "import uproot\n",
    "import awkward as ak\n",
    "import vector\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.constants as const\n",
    "import boost_histogram as bh\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras_tuner\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "m_e = const.electron_mass / 1000\n",
    "m_mu = 0.105658\n",
    "\n",
    "def import_data():\n",
    "    tree_plus_0 = uproot.open('../Delphes/delphes_output_WWjj_e+mu_smeft_cWtil_NP0.root:Delphes')\n",
    "    plus_events_0 = tree_plus_0.arrays(['Event.Weight', 'Electron_size', 'Electron.Eta', 'Electron.PT',\n",
    "                           'Electron.Phi', 'Muon_size', 'Muon.PT', 'Muon.Eta', 'Muon.Phi',\n",
    "                           'Jet_size', 'Jet.PT', 'Jet.Eta', 'Jet.Phi','Jet.Mass','Jet.BTag',])\n",
    "    \n",
    "    tree_plus_1 = uproot.open('../Delphes/output_WWjj_e+mu_smeft_cWtil_NP1_tot.root:Delphes')\n",
    "    plus_events_1 = tree_plus_1.arrays(['Event.Weight', 'Electron_size', 'Electron.Eta', 'Electron.PT',\n",
    "                           'Electron.Phi', 'Muon_size', 'Muon.PT', 'Muon.Eta', 'Muon.Phi',\n",
    "                           'Jet_size', 'Jet.PT', 'Jet.Eta', 'Jet.Phi','Jet.Mass','Jet.BTag',])\n",
    "    \n",
    "    tree_plus_2 = uproot.open('../Delphes/delphes_output_WWjj_e+mu_smeft_cWtil_NP2.root:Delphes')\n",
    "    plus_events_2 = tree_plus_2.arrays(['Event.Weight', 'Electron_size', 'Electron.Eta', 'Electron.PT',\n",
    "                           'Electron.Phi', 'Muon_size', 'Muon.PT', 'Muon.Eta', 'Muon.Phi',\n",
    "                           'Jet_size', 'Jet.PT', 'Jet.Eta', 'Jet.Phi','Jet.Mass','Jet.BTag',])\n",
    "    \n",
    "    limited_tree_plus_0 = uproot.open('../Delphes/delphes_output_WWjj_e+mu_smeft_cWtil_NP0_m_jj=200_tot.root:Delphes')\n",
    "    limited_plus_events_0 = limited_tree_plus_0.arrays(['Event.Weight', 'Electron_size', 'Electron.Eta', 'Electron.PT',\n",
    "                           'Electron.Phi', 'Muon_size', 'Muon.PT', 'Muon.Eta', 'Muon.Phi',\n",
    "                           'Jet_size', 'Jet.PT', 'Jet.Eta', 'Jet.Phi','Jet.Mass','Jet.BTag',])\n",
    "    \n",
    "    limited_tree_plus_1 = uproot.open('../Delphes/output_WWjj_e+mu_smeft_cWtil_NP1_m_jj=200_tot.root:Delphes')\n",
    "    limited_plus_events_1 = limited_tree_plus_1.arrays(['Event.Weight', 'Electron_size', 'Electron.Eta', 'Electron.PT',\n",
    "                           'Electron.Phi', 'Muon_size', 'Muon.PT', 'Muon.Eta', 'Muon.Phi',\n",
    "                           'Jet_size', 'Jet.PT', 'Jet.Eta', 'Jet.Phi','Jet.Mass','Jet.BTag',])\n",
    "    \n",
    "    limited_tree_plus_2 = uproot.open('../Delphes/delphes_output_WWjj_e+mu_smeft_cWtil_NP2_m_jj=200.root:Delphes')\n",
    "    limited_plus_events_2 = limited_tree_plus_2.arrays(['Event.Weight', 'Electron_size', 'Electron.Eta', 'Electron.PT',\n",
    "                           'Electron.Phi', 'Muon_size', 'Muon.PT', 'Muon.Eta', 'Muon.Phi',\n",
    "                           'Jet_size', 'Jet.PT', 'Jet.Eta', 'Jet.Phi','Jet.Mass','Jet.BTag',])\n",
    "\n",
    "    total_plus_events_0 = ak.concatenate([plus_events_0, limited_plus_events_0], mergebool=True, highlevel=True)\n",
    "    total_plus_events_1 = ak.concatenate([plus_events_1, limited_plus_events_1], mergebool=True, highlevel=True)\n",
    "    total_plus_events_2 = ak.concatenate([plus_events_2, limited_plus_events_2], mergebool=True, highlevel=True)\n",
    "\n",
    "    print(\"limited interfernece:\", len(limited_plus_events_1),\"unlimited interference:\", len(plus_events_1))\n",
    "    print(\"limited SM:\", len(limited_plus_events_0),\"unlimited SM:\", len(plus_events_0))\n",
    "    return total_plus_events_0, total_plus_events_1, total_plus_events_2\n",
    "\n",
    "def di_invariant_mass_calc(particles):\n",
    "    diparticle_mass = (particles[:, 0] + particles[:, 1]).mass  # calculates invariant mass for electron-electron or muon-muon pairs\n",
    "    return diparticle_mass\n",
    "\n",
    "def cross_invariant_mass_calc(particle_1, particle_2):\n",
    "    diparticle_mass = (particle_1[:, 0] + particle_2[:, 0]).mass  # calculates invariant mass for electron-muon pairs\n",
    "    return diparticle_mass\n",
    "\n",
    "def delta_eta(particles):\n",
    "    return np.abs(particles[:, 0].eta - particles[:, 1].eta)\n",
    "\n",
    "def di_transverse_momentum(particle_1, *particle_2):\n",
    "    if len(particle_2) == 0:\n",
    "        diparticle_momentum = (particle_1[:, 0] + particle_1[:, 1]).pt\n",
    "    else:\n",
    "        diparticle_momentum = (particle_1[:, 0] + particle_2[0][:, 0]).pt\n",
    "    print(np.max(diparticle_momentum))\n",
    "    print(np.min(diparticle_momentum))\n",
    "    return diparticle_momentum\n",
    "\n",
    "def electron_muon_cuts(events, cross_section, k_factor):\n",
    "    cut_flow_counts = {\n",
    "        'Initial': len(events),\n",
    "        'Single Lepton Selection': 0,\n",
    "        'Pt and Eta Lepton Cuts': 0,\n",
    "        'Highest Pt Lepton': 0,\n",
    "        'Invariant Mass Lepton Cuts': 0,\n",
    "        'Jet Size': 0,\n",
    "        'Pt and Eta Jet Cuts': 0,\n",
    "        'Delta Eta Jet Cut': 0,\n",
    "        'Invariant Mass Jet Cut': 0\n",
    "        }\n",
    "    weights = events['Event.Weight']\n",
    "\n",
    "    single_electron_muon_mask = (events['Electron_size'] == 1) & (events['Muon_size'] == 1)\n",
    "    unfiltered_events = events[single_electron_muon_mask]\n",
    "    cut_flow_counts['Single Lepton Selection'] = len(unfiltered_events)\n",
    "\n",
    "    electron_pt = unfiltered_events['Electron.PT']\n",
    "    electron_eta = unfiltered_events['Electron.Eta']\n",
    "    muon_pt = unfiltered_events['Muon.PT']\n",
    "    muon_eta = unfiltered_events['Muon.Eta']\n",
    "\n",
    "    electron_pt_mask = electron_pt > 20\n",
    "    electron_eta_mask = np.abs(electron_eta) < 2.47\n",
    "    exclusion_mask = (np.abs(electron_eta) < 1.37) | (np.abs(electron_eta) > 1.52)\n",
    "    total_electron_mask = electron_pt_mask & electron_eta_mask & exclusion_mask\n",
    "    muon_pt_mask = muon_pt > 20\n",
    "    muon_eta_mask = np.abs(muon_eta) < 2.5\n",
    "    total_muon_mask = muon_pt_mask & muon_eta_mask\n",
    "\n",
    "    filtered_mask = (ak.sum(total_electron_mask, axis=1) == 1) & (ak.sum(total_muon_mask, axis=1) == 1)\n",
    " \n",
    "    cut_flow_counts['Pt and Eta Lepton Cuts'] = len(unfiltered_events[filtered_mask])\n",
    "\n",
    "    highest_pt_lepton_mask = (electron_pt >= muon_pt)\n",
    "    highest_pt_cut = 27\n",
    "    highest_pt_mask = ((highest_pt_lepton_mask & (electron_pt > highest_pt_cut)) | \n",
    "                       (~highest_pt_lepton_mask & (muon_pt > highest_pt_cut)))\n",
    "    \n",
    "    total_electron_mask = total_electron_mask & highest_pt_mask\n",
    "    filtered_mask = (ak.sum(total_electron_mask, axis=1) == 1) & (ak.sum(total_muon_mask, axis=1) == 1)\n",
    "    \n",
    "    filtered_events = unfiltered_events[filtered_mask]\n",
    "    filtered_weights = weights[filtered_mask]\n",
    "\n",
    "    cut_flow_counts['Highest Pt Lepton'] = len(filtered_events)\n",
    "\n",
    "    filtered_electrons = electron_vector(filtered_events)\n",
    "    filtered_muons = muon_vector(filtered_events)\n",
    "\n",
    "    filtered_electron_muon_mass = cross_invariant_mass_calc(filtered_electrons, filtered_muons)\n",
    "    electron_muon_invariant_mass_mask = (filtered_electron_muon_mass > 40) & (filtered_electron_muon_mass < 400)\n",
    "    filtered_electron_muon_mass = filtered_electron_muon_mass[electron_muon_invariant_mass_mask]\n",
    "\n",
    "    filtered_events = filtered_events[electron_muon_invariant_mass_mask]\n",
    "    filtered_weights = filtered_weights[electron_muon_invariant_mass_mask]\n",
    "    cut_flow_counts['Invariant Mass Lepton Cuts'] = len(filtered_events)\n",
    "\n",
    "    return filtered_events, filtered_weights, cut_flow_counts\n",
    "\n",
    "def jet_cuts(filtered_events, events, filtered_weights, k_factor, cut_flow_counts):\n",
    "    jet_size_mask = filtered_events['Jet_size'] >= 2\n",
    "    cut_flow_counts['Jet Size'] = len(filtered_events[jet_size_mask])\n",
    "\n",
    "    jet_b_tag = filtered_events['Jet.BTag'] == 0\n",
    "    jet_pt = filtered_events['Jet.PT']\n",
    "    jet_eta = filtered_events['Jet.Eta']\n",
    "\n",
    "    forward_jet_mask = ((np.abs(filtered_events['Jet.Eta']) > 2.5) & (np.abs(filtered_events['Jet.Eta']) < 4.5) & (filtered_events['Jet.PT'] > 30)) & jet_b_tag\n",
    "    central_jet_mask = ((np.abs(filtered_events['Jet.Eta']) < 2.5) & (filtered_events['Jet.PT'] > 20)) & jet_b_tag\n",
    "    central_jet_region_mask = (abs(filtered_events['Jet.Eta']) < 2.5)\n",
    "\n",
    "    total_forward_central_mask = forward_jet_mask | central_jet_mask\n",
    "    filtered_central_jet_mask = (ak.sum(central_jet_region_mask, axis=1) >= 2) & ak.all(total_forward_central_mask, axis=1)\n",
    "    cut_flow_counts['Pt and Eta Jet Cuts'] = len(filtered_events[filtered_central_jet_mask])\n",
    "\n",
    "    jet_eta = filtered_events['Jet.Eta'][filtered_central_jet_mask]\n",
    "    jet_pt = filtered_events['Jet.PT'][filtered_central_jet_mask]\n",
    "    jet_phi = filtered_events['Jet.Phi'][filtered_central_jet_mask]\n",
    "    jet_mass = filtered_events['Jet.Mass'][filtered_central_jet_mask]\n",
    "\n",
    "    jet_pt_masked = jet_pt.mask[jet_eta < 2.5]\n",
    "    jet_eta_masked = jet_eta.mask[jet_eta < 2.5]\n",
    "    jet_phi_masked = jet_phi.mask[jet_eta < 2.5]\n",
    "    jet_mass_masked = jet_mass.mask[jet_eta < 2.5]\n",
    "    jet_pt_masked = ak.drop_none(jet_pt_masked, highlevel=True)\n",
    "    jet_eta_masked = ak.drop_none(jet_eta_masked, highlevel=True)\n",
    "    jet_phi_masked = ak.drop_none(jet_phi_masked, highlevel=True)\n",
    "    jet_mass_masked = ak.drop_none(jet_mass_masked, highlevel=True)\n",
    "\n",
    "    jets_central = vector.zip({\n",
    "        'pt':jet_pt_masked,\n",
    "        'eta':jet_eta_masked,\n",
    "        'phi':jet_phi_masked,\n",
    "        'mass':jet_mass_masked\n",
    "    })\n",
    "\n",
    "    central_jet_mass = di_invariant_mass_calc(jets_central)\n",
    "    central_jet_eta = delta_eta(jets_central)\n",
    "\n",
    "    jets_central_mass_cut_mask = (central_jet_mass < 160)\n",
    "    cut_flow_counts['Invariant Mass Jet Cut'] = len(filtered_events[jets_central_mass_cut_mask])\n",
    "    jets_central_delta_eta_cut_mask = (central_jet_eta < 1.5)\n",
    "    cut_flow_counts['Delta Eta Jet Cut'] = len(filtered_events[jets_central_delta_eta_cut_mask])\n",
    "\n",
    "    central_jet_cut_mask = jets_central_mass_cut_mask & jets_central_delta_eta_cut_mask\n",
    "\n",
    "    filtered_events = filtered_events[filtered_central_jet_mask]\n",
    "    filtered_events = filtered_events[central_jet_cut_mask]\n",
    "    filtered_weights = filtered_weights[filtered_central_jet_mask]\n",
    "    filtered_weights = filtered_weights[central_jet_cut_mask]\n",
    "\n",
    "    weight_normalisation = NP_normalisation(events, k_factor)\n",
    "    filtered_weights = (weight_normalisation * np.ones_like(filtered_weights)) * filtered_weights\n",
    "\n",
    "    print(\"Event yield is:\", np.sum(filtered_weights) ,\".\")\n",
    "\n",
    "    return filtered_events, filtered_weights\n",
    "\n",
    "def electron_vector(filtered_events):\n",
    "    electrons = vector.zip({\n",
    "        'pt': filtered_events['Electron.PT'],\n",
    "        'eta': filtered_events['Electron.Eta'],\n",
    "        'phi': filtered_events['Electron.Phi'],\n",
    "        'mass': m_e})\n",
    "    electrons = ak.drop_none(electrons, highlevel=True)\n",
    "    \n",
    "    return electrons\n",
    "\n",
    "def muon_vector(filtered_events):\n",
    "    muons = vector.zip({\n",
    "        'pt': filtered_events['Muon.PT'],\n",
    "        'eta': filtered_events['Muon.Eta'],\n",
    "        'phi': filtered_events['Muon.Phi'],\n",
    "        'mass': m_mu})\n",
    "    muons = ak.drop_none(muons, highlevel=True)\n",
    "    \n",
    "    return muons\n",
    "\n",
    "def lepton_vector(filtered_events):\n",
    "    electrons = electron_vector(filtered_events)\n",
    "    muons = muon_vector(filtered_events)\n",
    "\n",
    "    combined_particles = ak.concatenate([muons, electrons], axis=1, highlevel=True)\n",
    "    combined_ordered_particles = combined_particles[ak.argsort(combined_particles.pt, ascending=False)]\n",
    "    \n",
    "    return combined_ordered_particles\n",
    "\n",
    "def jet_vector(filtered_events):\n",
    "    jets = vector.zip({\n",
    "        'pt': filtered_events['Jet.PT'],\n",
    "        'eta': filtered_events['Jet.Eta'],\n",
    "        'phi': filtered_events['Jet.Phi'],\n",
    "        'mass': filtered_events['Jet.Mass']})\n",
    "    jets = ak.drop_none(jets, highlevel=True)\n",
    "    jets = jets[ak.argsort(jets.pt, ascending=False)]\n",
    "\n",
    "    return jets\n",
    "\n",
    "def NP_normalisation(events, k_factor):\n",
    "    luminosity = 139 # fb^-1\n",
    "    normalised_weight = (1 / len(events)) * luminosity * k_factor\n",
    "\n",
    "    return normalised_weight\n",
    "\n",
    "def bin_widths(histogram):\n",
    "    bin_width = histogram[1:] - histogram[:-1]\n",
    "    return bin_width\n",
    "\n",
    "def chi_squared(observed, expected, uncertainty):\n",
    "    chi_squared = np.sum(((observed - expected) / uncertainty) ** 2)\n",
    "    return chi_squared\n",
    "\n",
    "def ak_to_numpy_array(root_observable):\n",
    "    nested_counts = ak.num(root_observable)\n",
    "    max_len = ak.max(nested_counts)\n",
    "\n",
    "    root_observable = ak.pad_none(root_observable, target=max_len, axis=1)\n",
    "    root_observable = ak.fill_none(root_observable, 0)\n",
    "    root_observable = ak.to_numpy(root_observable)\n",
    "\n",
    "    return root_observable\n",
    "\n",
    "def root_variable_selector(filtered_events, filtered_weights):\n",
    "    print(\"Number of events:\", len(filtered_weights))\n",
    "    jet_eta = filtered_events['Jet.Eta']\n",
    "    jet_phi = filtered_events['Jet.Phi']\n",
    "    electron_eta = filtered_events['Electron.Eta']\n",
    "    electron_phi = filtered_events['Electron.Phi']\n",
    "    muon_eta = filtered_events['Muon.Eta']\n",
    "    muon_phi = filtered_events['Muon.Phi']\n",
    "\n",
    "    jet_eta = ak_to_numpy_array(jet_eta)\n",
    "    jet_phi = ak_to_numpy_array(jet_phi)\n",
    "    electron_eta = ak_to_numpy_array(electron_eta)\n",
    "    electron_phi = ak_to_numpy_array(electron_phi)\n",
    "    muon_eta = ak_to_numpy_array(muon_eta)\n",
    "    muon_phi = ak_to_numpy_array(muon_phi)\n",
    "    weight_data = ak_to_numpy_array(filtered_weights)\n",
    "\n",
    "    x_data = np.concatenate([jet_eta, jet_phi, electron_eta, electron_phi, muon_eta, muon_phi], axis=-1)\n",
    "\n",
    "    y_data = np.array(filtered_weights)\n",
    "    y_data = (y_data > 0).astype(int)\n",
    "    y_data = y_data\n",
    "    \n",
    "    print(x_data)\n",
    "\n",
    "    return x_data, y_data, weight_data\n",
    "\n",
    "def build_ML_model(hp, *args):\n",
    "    input_shape = args[0]\n",
    "\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.Input(shape=input_shape))\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(units=hp.Int('units', min_value=16, max_value=512, step=1), activation='relu'))\n",
    "    model.add(keras.layers.Dropout(rate=hp.Float('dropout', 0.1, 0.3, step=0.01)))\n",
    "    model.add(keras.layers.Dense(2, activation='sigmoid'))\n",
    "\n",
    "    loss_fn = keras.losses.BinaryCrossentropy()\n",
    "    model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def ML_training(x_data, y_data, weight_data):\n",
    "    x_train, x_test, y_train, y_test, train_weights, test_weights = train_test_split(x_data, y_data, weight_data, test_size=0.2, random_state=42)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    x_train = scaler.fit_transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "\n",
    "    model = keras_tuner.RandomSearch(\n",
    "        lambda hp: build_ML_model(hp, x_train.shape[1:]),\n",
    "        objective='val_loss',\n",
    "        max_trials=5,\n",
    "        directory='keras_tuner_2',\n",
    "        project_name='keras_training'\n",
    "    )\n",
    "    \n",
    "    model.search(x_train, y_train, sample_weight=train_weights, epochs=10, validation_data=(x_test, y_test, test_weights))\n",
    "    best_model = model.get_best_models(num_models=1)[0]\n",
    "    best_model.evaluate(x_test, y_test, sample_weight=test_weights, verbose=2)\n",
    "    \n",
    "    best_hp = model.get_best_hyperparameters(num_trials=1)[0]\n",
    "    print(\"Best hyperparameters:\", best_hp.values)\n",
    "\n",
    "    best_model.save(\"tensor_flow_2.keras\")\n",
    "    final_results = best_model.predict(x_data)\n",
    "    #probability_model = tf.keras.Sequential([best_model, tf.keras.layers.softmax()])\n",
    "    final_results = best_model.predict(x_data)\n",
    "    print(\"layer activation\", best_model.layers[-1].activation)\n",
    "    print(final_results)\n",
    "\n",
    "    #plot_loss_function(model)\n",
    "\n",
    "    return final_results \n",
    "\n",
    "def plot_loss_function(model):\n",
    "    best_trial = model.oracle.get_best_trials(num_trials=1)[0]\n",
    "    train_loss = best_trial.metrics.get_history('loss')\n",
    "    val_loss = best_trial.metrics.get_history('val_loss')\n",
    "    \n",
    "    epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(epochs, train_loss, label='Training Loss', marker='o')\n",
    "    plt.plot(epochs, val_loss, label='Validation Loss', marker='s')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss Function Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "def basic_ML_training(x_data, y_data, weight_data):\n",
    "    x_train, x_test, y_train, y_test, train_weights, test_weights = train_test_split(x_data, y_data, weight_data, test_size=0.2, random_state=42)\n",
    "\n",
    "    model = tf.keras.models.Sequential([ \n",
    "        tf.keras.Input(shape=(x_train.shape[1],)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(256, activation='relu'), \n",
    "        tf.keras.layers.Dropout(0.2), \n",
    "        tf.keras.layers.Dense(2) \n",
    "    ])\n",
    "\n",
    "    predictions = model(x_train[:1]).numpy() \n",
    "    predictions\n",
    "\n",
    "    tf.nn.softmax(predictions).numpy()\n",
    "\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "    loss_fn(y_train[:1], predictions).numpy()\n",
    "\n",
    "    model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy']) \n",
    "\n",
    "    model.fit(x_train, y_train, sample_weight=train_weights, epochs=10) \n",
    "    model.evaluate(x_test, y_test, sample_weight=test_weights, verbose=2)\n",
    "    model.save(\"tensor_flow_1.keras\")\n",
    "\n",
    "    predictions = model(x_train[:1]).numpy() \n",
    "    predictions\n",
    "\n",
    "    tf.nn.softmax(predictions).numpy()\n",
    "\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "    loss_fn(y_train[:1], predictions).numpy()\n",
    "\n",
    "    probability_model = tf.keras.Sequential([model, tf.keras.layers.Softmax()])\n",
    "    final_results = probability_model.predict(x_data)\n",
    "\n",
    "    return final_results \n",
    "\n",
    "def load_ML_model(x_data):\n",
    "    loaded_model = tf.keras.models.load_model(\"tensor_flow_2.keras\")\n",
    "    final_results = loaded_model.predict(x_data)\n",
    "\n",
    "    return final_results \n",
    "\n",
    "def plot_ML_observable(ML_results, weight_data):\n",
    "    scale_factor = 1\n",
    "    ML_observable = ML_results[:, 1] + ML_results[:, 0]\n",
    "    \n",
    "    bin_number = 6\n",
    "    range_limit = (np.min(ML_observable), np.max(ML_observable))\n",
    "\n",
    "    histogram_0 = bh.Histogram(bh.axis.Regular(bin_number, *range_limit, underflow=False, overflow=False))\n",
    "    histogram_0.fill(ML_observable, weight=weight_data)\n",
    "    \n",
    "    bin_widths = histogram_0.axes[0].widths\n",
    "    differential_cross_section_0 = histogram_0.view() / bin_widths\n",
    "    \n",
    "    hist_0_squared = bh.Histogram(histogram_0.axes[0])\n",
    "    hist_0_squared.fill(ML_observable, weight=weight_data**2)\n",
    "\n",
    "    uncertainty_0 = np.sqrt(hist_0_squared.view()) / bin_widths\n",
    "\n",
    "    bin_centers = histogram_0.axes[0].centers\n",
    "    bin_edges = histogram_0.axes[0].edges\n",
    "\n",
    "    chi_squared_1 = chi_squared(differential_cross_section_0, 0, uncertainty_0)\n",
    "    print(\"chi_squared_1 about y=0:\", chi_squared_1)\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    plt.hist(bin_edges[:-1], bins=bin_edges, weights=differential_cross_section_0 /scale_factor, label=r\"$O_{NN}$\", color='mediumblue', histtype='step')\n",
    "    plt.errorbar(bin_centers, differential_cross_section_0 / scale_factor, yerr=uncertainty_0 /scale_factor, fmt='None', color='mediumblue')\n",
    "\n",
    "    plt.title(r'$O_{NN}$ for $WWW$ region', fontsize=14)\n",
    "    plt.xlabel(r'$d\\sigma/dO_{NN}$', fontsize=14)\n",
    "    plt.ylabel(r'$O_{NN}$ [fb]', fontsize=14)\n",
    "    y_min, y_max = plt.ylim()\n",
    "    plt.ylim(y_min, y_max + 0.20 * (y_max - y_min))\n",
    "    plt.xticks(fontsize=12)  \n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.legend(frameon=False, fontsize=12)\n",
    "    plt.savefig(f\"O_NN_1.png\", dpi=1000, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    fractional_uncertainty_0 = np.divide(\n",
    "        uncertainty_0, differential_cross_section_0,\n",
    "        out=np.zeros_like(uncertainty_0),\n",
    "        where=differential_cross_section_0 != 0\n",
    "    )\n",
    "\n",
    "    print(\"O_NN Fractional Uncertainty:\", fractional_uncertainty_0)\n",
    "\n",
    "def print_branches(tree):\n",
    "    keys = tree.keys()\n",
    "    for key in keys:\n",
    "        print(key)\n",
    "    \n",
    "def __main__():\n",
    "    plus_events_0, plus_events_1, plus_events_2 = import_data()\n",
    "    print(\"Plus 1:\", len(plus_events_1['Event.Weight']))\n",
    "    plus_cross_section = 2.486\n",
    "    minus_cross_section = 1.096\n",
    "    plus_k_factor = 136 / 73.69\n",
    "    minus_k_factor = 76 / 39.26\n",
    "\n",
    "    plus_filtered_events_0, filtered_weights_0, plus_cut_flow_counts_0 = electron_muon_cuts(plus_events_0, plus_cross_section, plus_k_factor)\n",
    "    plus_filtered_events_1, filtered_weights_1, plus_cut_flow_counts_1 = electron_muon_cuts(plus_events_1, plus_cross_section, plus_k_factor)\n",
    "    plus_filtered_events_2, filtered_weights_2, plus_cut_flow_counts_2 = electron_muon_cuts(plus_events_2, plus_cross_section, plus_k_factor)\n",
    "    \n",
    "    plus_final_filtered_events_0, plus_final_filtered_weights_0 = jet_cuts(plus_filtered_events_0, plus_events_0, filtered_weights_0, plus_k_factor, plus_cut_flow_counts_0)\n",
    "    plus_final_filtered_events_1, plus_final_filtered_weights_1 = jet_cuts(plus_filtered_events_1, plus_events_1, filtered_weights_1, plus_k_factor, plus_cut_flow_counts_1)\n",
    "    plus_final_filtered_events_2, plus_final_filtered_weights_2 = jet_cuts(plus_filtered_events_2, plus_events_2, filtered_weights_2, plus_k_factor, plus_cut_flow_counts_2)\n",
    "    \n",
    "    x_data_1, y_data_1, weight_data_1 = root_variable_selector(plus_final_filtered_events_1, plus_final_filtered_weights_1)\n",
    "    ML_results_1 = ML_training(x_data_1, y_data_1, weight_data_1)\n",
    "    plot_ML_observable(ML_results_1, weight_data_1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    __main__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
